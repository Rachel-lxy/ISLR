---
title: "Chapter 4 Lab"
author: "Xinyu Li"
date: "January 17, 2017"
output: html_document
---

## 4.6.1 The Stock Market Data
```{r}
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
cor(Smarket[,-9])
attach(Smarket)
plot(Year, Volume)
```

## 4.6.2 Logistic Regression
Train and obtain the error rate on the entire dataset
```{r}
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)
summary(glm.fit)
coef(glm.fit)
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
contrasts(Direction)
glm.probs = predict(glm.fit, type="response")
glm.probs[1:10]
glm.pred = rep("Down", 1250)
glm.pred[glm.probs>.5] = "Up"
table(glm.pred, Direction)
mean(glm.pred==Direction) # test correct rate
mean(glm.pred!=Direction) # test error rate
```

Train on data before 2005 and test on data in 2005, and obtain the error rate on test dataset
```{r}
train = (Year<2005)
Smarket.2005 = Smarket[!train,]
dim(Smarket.2005)
Direction.2005 = Direction[!train]
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type="response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs>.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005) # test correct rate
mean(glm.pred!=Direction.2005) # test error rate
```

Use only Lag1 and Lag2 to build the logistic regression model
```{r}
glm.fit = glm(Direction~Lag1+Lag2, data=Smarket, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type="response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs>.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005) # test correct rate
mean(glm.pred!=Direction.2005) # test error rate
```

Predict the responses based on new values of Lag1 and Lag2
```{r}
predict(glm.fit, newdata=data.frame(Lag1=c(1.2,1.5), Lag2=c(1.1,-0.8)), type="response")
```

## 4.6.3 Linear Discriminant Analysis
```{r}
library(MASS)
lda.fit = lda(Direction~Lag1+Lag2, data=Smarket, subset=train)
# coefficients provides the linear combination of Lag1 and Lag2 that are used to form the decision rule
# when -0.642*Lag1-0.514*Lag2 is large, LDA will predict a market increase, and if it is small, LDA will predict a market decline
lda.fit 
plot(lda.fit)
lda.pred = predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class==Direction.2005)
sum(lda.pred$posterior[,1]>.5)
sum(lda.pred$posterior[,1]<.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
sum(lda.pred$posterior[,1]>.9)
```

## 4.6.4 Quadratic Discriminant Analysis
```{r}
qda.fit = qda(Direction~Lag1+Lag2, data=Smarket, subset=train)
# QDA involves a quadratic rather than linear function of predictors, thus the coefficients are not returned as LDA
qda.fit
qda.pred = predict(qda.fit, Smarket.2005)
qda.class = qda.pred$class
table(qda.class, Direction.2005)
mean(qda.class==Direction.2005)
```

## 4.6.5 K-Nearest Neighbors
```{r}
library(class)
train.X = cbind(Lag1, Lag2)[train,]
test.X = cbind(Lag1, Lag2)[!train,]
train.Direction = Direction[train]
# R will randomly break the tie if several observations are tied as nearest neighbors
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, Direction.2005)
mean(knn.pred==Direction.2005)
knn.pred = knn(train.X, test.X, train.Direction, k=3)
table(knn.pred, Direction.2005)
mean(knn.pred==Direction.2005)
```

## 4.6.6 An Application to Caravan Insurance Data
KNN with k=1
```{r}
dim(Caravan)
attach(Caravan)
summary(Purchase)
348/5822
# normalize the data first as the variables are on very different scale
# normalization can prevent large-scale variables to dominate the results of knn
standardized.X = scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test = 1:1000
train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = Purchase[-test]
test.Y = Purchase[test]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
table(knn.pred, test.Y)
9/(68+9)
```

KNN with k=3
```{r}
knn.pred = knn(train.X, test.X, train.Y, k=3)
mean(test.Y!=knn.pred)
table(knn.pred, test.Y)
5/(21+5)
```

KNN with k=5
```{r}
knn.pred = knn(train.X, test.X, train.Y, k=5)
mean(test.Y!=knn.pred)
table(knn.pred, test.Y)
4/(11+4)
```

logistic regression
```{r}
glm.fit = glm(Purchase~., data=Caravan, family=binomial, subset=-test)
glm.probs = predict(glm.fit, Caravan[test,], type="response")
glm.pred = rep("No", 1000)
glm.pred[glm.probs>.5] = "Yes"
mean(glm.pred==test.Y)
table(glm.pred, test.Y)
glm.pred = rep("No", 1000)
glm.pred[glm.probs>.25] = "Yes"
mean(glm.pred==test.Y)
table(glm.pred, test.Y)
11/(22+11)
```